### Edyoda-Assignment--NLP-

# 1. SMS Spam Collection Dataset

The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.

Dataset Link: https://www.kaggle.com/uciml/sms-spam-collection-dataset

# 2. CommonLit Readability Prize

Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.

In this competition, we're predicting the reading ease of excerpts from literature. We've provided excerpts from several time periods and a wide range of reading ease scores. Note that the test set includes a slightly larger proportion of modern texts (the type of texts we want to generalize to) than the training set.

Dataset Link: https://www.kaggle.com/c/commonlitreadabilityprize/data

Study reference material:

https://www.kaggle.com/ruchi798/commonlit-readability-prize-eda-baseline

https://www.kaggle.com/manishkc06/text-pre-processing-data-wrangling


# 3. Reddit Comment Score Prediction

Reddit is a social news platform that allows users to discuss and vote on content that other users have submitted.On an average reddit receives 470,000 comments per day. The comments are further upvoted or downvoted by the registered users. 

Imagine you are going to start a forum where users can post or comment or share content on the platform. Now you want to filter out some positive comments and recommend them to your users. 

Build a machine learning model that will help you know which comment or content is going to be popular in the near future (the content which receives the highest upvotes will be popular) and accordingly recommend such content to your users.

Dataset Link: https://dphi.tech/challenges/data-sprint-36-reddit-comment-score-prediction/89/data


